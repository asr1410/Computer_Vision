{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# store accuracy and loss for each epoch\n",
    "train_losses1 = []\n",
    "val_losses1 = []\n",
    "train_accuracies1 = []\n",
    "val_accuracies1 = []\n",
    "\n",
    "def load_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Read the magic number and dimensions\n",
    "        magic_number, num_items = np.frombuffer(f.read(8), dtype='>u4')\n",
    "\n",
    "        # Handle different data types\n",
    "        if magic_number == 2051:  # Image data\n",
    "            num_rows, num_cols = np.frombuffer(f.read(8), dtype='>u4')\n",
    "            data = np.frombuffer(f.read(), dtype=np.uint8).reshape(num_items, num_rows, num_cols)\n",
    "        elif magic_number == 2049:  # Label data\n",
    "            data = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown magic number: {magic_number}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def load_mnist():\n",
    "    X_train = load_idx('train-images.idx3-ubyte')\n",
    "    y_train = load_idx('train-labels.idx1-ubyte')\n",
    "    X_test = load_idx('t10k-images.idx3-ubyte')\n",
    "    y_test = load_idx('t10k-labels.idx1-ubyte')\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def one_hot_encode(y):\n",
    "    num_classes = np.max(y) + 1\n",
    "    matrix = np.eye(num_classes, dtype=int)[y]\n",
    "    return matrix\n",
    "\n",
    "def preprocess_data(X, y):\n",
    "    X_normalized = X / 255.0\n",
    "    num_samples = X.shape[0]\n",
    "    X_flattened = X_normalized.reshape(num_samples, -1)\n",
    "    y_encoded = one_hot_encode(y)\n",
    "    return X_flattened, y_encoded\n",
    "\n",
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    np.random.seed(42)\n",
    "    weights_hidden = np.random.randn(input_size, hidden_size)\n",
    "    bias_hidden = np.zeros((1, hidden_size))\n",
    "    weights_output = np.random.randn(hidden_size, output_size)\n",
    "    bias_output = np.zeros((1, output_size))\n",
    "    return weights_hidden, bias_hidden, weights_output, bias_output\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "def forward_pass(X, weights_hidden, bias_hidden, weights_output, bias_output):\n",
    "    hidden_output = sigmoid(np.dot(X, weights_hidden) + bias_hidden)\n",
    "    final_output = softmax(np.dot(hidden_output, weights_output) + bias_output)\n",
    "    return hidden_output, final_output\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def backward_pass(X, y, hidden_output, final_output, weights_hidden, weights_output, learning_rate):\n",
    "    output_error = y - final_output\n",
    "    output_delta = output_error * final_output * (1 - final_output)\n",
    "    hidden_error = np.dot(output_delta, weights_output.T)\n",
    "    hidden_delta = hidden_error * hidden_output * (1 - hidden_output)\n",
    "    weights_hidden += learning_rate * np.dot(X.T, hidden_delta)\n",
    "    weights_output += learning_rate * np.dot(hidden_output.T, output_delta)\n",
    "    return weights_hidden, weights_output\n",
    "\n",
    "def predict(X, weights_hidden, bias_hidden, weights_output, bias_output):\n",
    "    _, final_output = forward_pass(X, weights_hidden, bias_hidden, weights_output, bias_output)\n",
    "    predictions = np.argmax(final_output, axis=1)\n",
    "    return predictions\n",
    "\n",
    "def calculate_accuracy(X, y, weights_hidden, bias_hidden, weights_output, bias_output):\n",
    "    predictions = predict(X, weights_hidden, bias_hidden, weights_output, bias_output)\n",
    "    true_labels = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == true_labels)\n",
    "    return accuracy\n",
    "\n",
    "def train_model(X_train, y_train, X_test, y_test, hidden_size, learning_rate, epochs):\n",
    "    input_size = X_train.shape[1]\n",
    "    output_size = y_train.shape[1]\n",
    "    weights_hidden, bias_hidden, weights_output, bias_output = initialize_parameters(input_size, hidden_size, output_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(X_train.shape[0]):  # SGD: process one sample at a time\n",
    "            xi = X_train[i:i+1]\n",
    "            yi = y_train[i:i+1]\n",
    "            hidden_output, final_output = forward_pass(xi, weights_hidden, bias_hidden, weights_output, bias_output)\n",
    "            loss = mse(yi, final_output)\n",
    "            train_losses1.append(loss)\n",
    "            weights_hidden, weights_output = backward_pass(xi, yi, hidden_output, final_output, weights_hidden, weights_output, learning_rate)\n",
    "        \n",
    "        train_accuracy = calculate_accuracy(X_train, y_train, weights_hidden, bias_hidden, weights_output, bias_output)\n",
    "        train_accuracies1.append(train_accuracy)\n",
    "        \n",
    "        # Calculate final output for validation set\n",
    "        _, val_final_output = forward_pass(X_test, weights_hidden, bias_hidden, weights_output, bias_output)\n",
    "        val_loss = mse(y_test, val_final_output)\n",
    "        val_losses1.append(val_loss)\n",
    "        val_accuracy = calculate_accuracy(X_test, y_test, weights_hidden, bias_hidden, weights_output, bias_output)\n",
    "        val_losses1.append(val_accuracy)\n",
    "\n",
    "        print(f'Epoch: {epoch+1}, Training Loss: {loss:.4f}, Training Accuracy: {train_accuracy:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "    \n",
    "    return weights_hidden, bias_hidden, weights_output, bias_output\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_mnist()\n",
    "X_train, y_train = preprocess_data(X_train, y_train)\n",
    "X_test, y_test = preprocess_data(X_test, y_test)\n",
    "\n",
    "hidden_size = 128\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "weights_hidden, bias_hidden, weights_output, bias_output = train_model(X_train, y_train, X_test, y_test, hidden_size, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.decoder = nn.Linear(hidden_size, input_size)  # linear activation on output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Load the MNIST dataset\n",
    "X_train, _, X_test, _ = load_mnist()\n",
    "\n",
    "# Reshape and normalize the data\n",
    "X_train = X_train.reshape(-1, 28*28) / 255.\n",
    "X_test = X_test.reshape(-1, 28*28) / 255.\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "\n",
    "# Create DataLoader\n",
    "train_data = DataLoader(TensorDataset(X_train, X_train), batch_size=64, shuffle=True)\n",
    "\n",
    "# Create the model\n",
    "model = Autoencoder(784, 128)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):  # 100 epochs\n",
    "    for data in train_data:\n",
    "        img, _ = data\n",
    "        img = Variable(img)\n",
    "        # Forward pass\n",
    "        output = model(img)\n",
    "        loss = criterion(output, img)\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Load the MNIST dataset\n",
    "X_train, y_train, X_test, y_test = load_mnist()\n",
    "\n",
    "# Reshape and normalize the data\n",
    "X_train = X_train.reshape(-1, 28*28) / 255.\n",
    "X_test = X_test.reshape(-1, 28*28) / 255.\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train_one_hot = torch.from_numpy(one_hot_encode(y_train)).float()\n",
    "y_test_one_hot = torch.from_numpy(one_hot_encode(y_test)).float()\n",
    "\n",
    "# Create DataLoader\n",
    "train_data = DataLoader(TensorDataset(X_train, y_train_one_hot), batch_size=64, shuffle=True)\n",
    "val_data = DataLoader(TensorDataset(X_test, y_test_one_hot), batch_size=64, shuffle=True)\n",
    "\n",
    "# Create the model\n",
    "model = MLP(784, 128, 10)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Initialize lists to store losses and accuracies\n",
    "train_losses3 = []\n",
    "val_losses3 = []\n",
    "train_accuracies3 = []\n",
    "val_accuracies3 = []\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):  # 100 epochs\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    model.train()\n",
    "    for data, target in train_data:\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Calculate training loss and accuracy\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        _, true = torch.max(target.data, 1)\n",
    "        train_correct += (predicted == true).sum().item()\n",
    "\n",
    "    train_losses3.append(train_loss / len(train_data))\n",
    "    train_accuracies3.append(train_correct / len(X_train))\n",
    "\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_data:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            # Calculate validation loss and accuracy\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            _, true = torch.max(target.data, 1)\n",
    "            val_correct += (predicted == true).sum().item()\n",
    "\n",
    "    val_losses3.append(val_loss / len(val_data))\n",
    "    val_accuracies3.append(val_correct / len(X_test))\n",
    "\n",
    "    print(f'Epoch: {epoch+1}, Training Loss: {train_losses3[-1]:.4f}, Training Accuracy: {train_accuracies3[-1]:.4f}, Validation Loss: {val_losses3[-1]:.4f}, Validation Accuracy: {val_accuracies3[-1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load the MNIST dataset\n",
    "X_train, y_train, X_test, y_test = load_mnist()\n",
    "\n",
    "# Normalize the data and convert to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train).float() / 255\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "X_test = torch.from_numpy(X_test).float() / 255\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_one_hot = torch.nn.functional.one_hot(y_train, 10).float()\n",
    "y_test_one_hot = torch.nn.functional.one_hot(y_test, 10).float()\n",
    "\n",
    "# Create DataLoader\n",
    "train_data = DataLoader(TensorDataset(X_train, y_train_one_hot), batch_size=64, shuffle=True)\n",
    "val_data = DataLoader(TensorDataset(X_test, y_test_one_hot), batch_size=64, shuffle=True)\n",
    "\n",
    "# Define the autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(512, 28*28),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder = Autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(autoencoder.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(100):  # 100 epochs\n",
    "    for data, _ in train_data:\n",
    "        data = Variable(data.view(-1, 28*28))\n",
    "        # Forward pass\n",
    "        output = autoencoder(data)\n",
    "        loss = criterion(output, data)\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Extract the weights from the autoencoder\n",
    "autoencoder_weights = list(autoencoder.encoder.children())[0].weight.data, list(autoencoder.encoder.children())[2].weight.data\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(512, 256),  # Adjusted to match the autoencoder's encoder output\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(256, 10),  # Adjusted to match the MLP's second layer output\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        return self.layer3(x)\n",
    "\n",
    "# Initialize the MLP with the weights from the autoencoder\n",
    "mlp = MLP()\n",
    "mlp.layer1[0].weight.data = autoencoder_weights[0]\n",
    "mlp.layer2[0].weight.data = autoencoder_weights[1]  # No need to transpose\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(mlp.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Train the MLP\n",
    "train_losses4, val_losses4, train_accuracies4, val_accuracies4 = [], [], [], []\n",
    "for epoch in range(100):  # 100 epochs\n",
    "    train_loss, val_loss, train_correct, val_correct = 0, 0, 0, 0\n",
    "    mlp.train()\n",
    "    for data, target in train_data:\n",
    "        data = Variable(data.view(-1, 28*28))\n",
    "        target = Variable(target)\n",
    "        # Forward pass\n",
    "        output = mlp(data)\n",
    "        loss = criterion(output, target)\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        _, actual = torch.max(target.data, 1)\n",
    "        train_correct += (predicted == actual).sum().item()\n",
    "    train_losses4.append(train_loss/len(train_data))\n",
    "    train_accuracies4.append(train_correct/len(X_train))\n",
    "\n",
    "    # Validate the model\n",
    "    mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_data:\n",
    "            data = Variable(data.view(-1, 28*28))\n",
    "            target = Variable(target)\n",
    "            output = mlp(data)\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            _, actual = torch.max(target.data, 1)\n",
    "            val_correct += (predicted == actual).sum().item()\n",
    "        val_losses4.append(val_loss/len(val_data))\n",
    "        val_accuracies4.append(val_correct/len(X_test))\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_losses4[-1]:.4f}, Val Loss: {val_losses4[-1]:.4f}, Train Acc: {train_accuracies4[-1]:.4f}, Val Acc: {val_accuracies4[-1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses3, label='Model III Training Loss')\n",
    "plt.plot(val_losses3, label='Model III Validation Loss')\n",
    "plt.plot(train_losses4, label='Model IV Training Loss')\n",
    "plt.plot(val_losses4, label='Model IV Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies3, label='Model III Training Accuracy')\n",
    "plt.plot(val_accuracies3, label='Model III Validation Accuracy')\n",
    "plt.plot(train_accuracies4, label='Model IV Training Accuracy')\n",
    "plt.plot(val_accuracies4, label='Model IV Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*4*4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "# Load the MNIST dataset\n",
    "X_train, y_train, X_test, y_test = load_mnist()\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_data = torch.utils.data.DataLoader(list(zip(X_train, y_train)), batch_size=64, shuffle=True)\n",
    "val_data = torch.utils.data.DataLoader(list(zip(X_test, y_test)), batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize the LeNet model\n",
    "lenet = LeNet()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(lenet.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Train the LeNet\n",
    "train_losses5, val_losses5, train_accuracies5, val_accuracies5 = [], [], [], []\n",
    "for epoch in range(100):  # 100 epochs\n",
    "    train_loss, val_loss, train_correct, val_correct = 0, 0, 0, 0\n",
    "    lenet.train()\n",
    "    for data, target in train_data:\n",
    "        data = Variable(data.view(-1, 1, 28, 28))  # Adjusted for 2D convolution\n",
    "        target = Variable(target)  # Adjusted for CrossEntropyLoss\n",
    "        # Forward pass\n",
    "        output = lenet(data)\n",
    "        loss = criterion(output, target)\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        train_correct += (predicted == target).sum().item()\n",
    "    train_losses5.append(train_loss/len(train_data))\n",
    "    train_accuracies5.append(train_correct/len(X_train))\n",
    "\n",
    "    # Validate the model\n",
    "    lenet.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_data:\n",
    "            data = Variable(data.view(-1, 1, 28, 28))  # Adjusted for 2D convolution\n",
    "            target = Variable(target)  # Adjusted for CrossEntropyLoss\n",
    "            output = lenet(data)\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            val_correct += (predicted == target).sum().item()\n",
    "        val_losses5.append(val_loss/len(val_data))\n",
    "        val_accuracies5.append(val_correct/len(X_test))\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_losses5[-1]:.4f}, Val Loss: {val_losses5[-1]:.4f}, Train Acc: {train_accuracies5[-1]:.4f}, Val Acc: {val_accuracies5[-1]:.4f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.figure()\n",
    "plt.plot(train_accuracies1, label='Model I Training Accuracy')\n",
    "plt.plot(val_accuracies1, label='Model I Validation Accuracy')\n",
    "plt.plot(train_accuracies5, label='Lenet Training Accuracy')\n",
    "plt.plot(val_accuracies5, label='Lenet Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Lenet compared with Best of above (1-hidden layer neural network)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
